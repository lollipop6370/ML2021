{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM0VqpEzN6f1byZxkDxZQM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f3a3d37353894381b750fd73b58ec926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa0f41f6d181417997f3a74d1f1b2a84",
              "IPY_MODEL_75559963e1c44884b78e82352c2a4d8a",
              "IPY_MODEL_d0a1bef3f1284020a0604e4fba68e9bb"
            ],
            "layout": "IPY_MODEL_e89fa1a0efd3425d82e350e67418e3b6"
          }
        },
        "fa0f41f6d181417997f3a74d1f1b2a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06907a3c036e415ea95cf985699f7bfa",
            "placeholder": "​",
            "style": "IPY_MODEL_71304ce571ec4d6d80b64d6de6ed18bd",
            "value": "100%"
          }
        },
        "75559963e1c44884b78e82352c2a4d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9255fe3e37b40a9806ad27bce8b2514",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47fa2ebc4ddf4e30afc052cf9431537d",
            "value": 400
          }
        },
        "d0a1bef3f1284020a0604e4fba68e9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5adc140cf4f44aa19e8f2675168b9343",
            "placeholder": "​",
            "style": "IPY_MODEL_10478e8253894c92ac8120ee2d9073e1",
            "value": " 400/400 [02:02&lt;00:00,  3.48it/s]"
          }
        },
        "e89fa1a0efd3425d82e350e67418e3b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06907a3c036e415ea95cf985699f7bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71304ce571ec4d6d80b64d6de6ed18bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9255fe3e37b40a9806ad27bce8b2514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47fa2ebc4ddf4e30afc052cf9431537d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5adc140cf4f44aa19e8f2675168b9343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10478e8253894c92ac8120ee2d9073e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lollipop6370/ML2021/blob/main/hw12_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 12 - Reinforcement Learning"
      ],
      "metadata": {
        "id": "GaQeoqoSriDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 前置作業\n",
        "首先我們需要安裝必要的系統套件及 PyPi 套件。 gym 這個套件由 OpenAI 所提供，是一套用來開發與比較 Reinforcement Learning 演算法的工具包（toolkit）。 而其餘套件則是為了在 Notebook 中繪圖所需要的套件。"
      ],
      "metadata": {
        "id": "ZyNsW5JRrvBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE_Jyoexrg4r",
        "outputId": "ab7af7f2-5988-456a-abc3-969f4495a0ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\u001b[0m\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,340 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,561 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,703 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,532 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,253 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,024 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,748 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,053 kB]\n",
            "Fetched 30.6 MB in 8s (3,664 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "37 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.15).\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libglu1-mesa\n",
            "Suggested packages:\n",
            "  libgle3 python3-numpy\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libglu1-mesa python3-opengl\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 824 kB of archives.\n",
            "After this operation, 8,092 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n",
            "Fetched 824 kB in 0s (6,917 kB/s)\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 126319 files and directories currently installed.)\n",
            "Preparing to unpack .../freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package python3-opengl.\n",
            "Preparing to unpack .../python3-opengl_3.1.5+dfsg-1_all.deb ...\n",
            "Unpacking python3-opengl (3.1.5+dfsg-1) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up python3-opengl (3.1.5+dfsg-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "\u001b[33mWARNING: Skipping box2d as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping box2d-py as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting swig\n",
            "  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.1\n",
            "Collecting box2d-py==2.3.8\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2379415 sha256=fea311d35db12166825306246f8e0755347fa210ac314519e6a564ffce3a3785\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyvirtualdisplay, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install python3-opengl xvfb -y\n",
        "!pip uninstall box2d box2d-py -y\n",
        "!pip install swig\n",
        "!pip install box2d-py==2.3.8\n",
        "!pip install gymnasium[box2d] --no-deps\n",
        "!pip install pyvirtualdisplay tqdm numpy torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接下來，設置好 virtual display，並引入所有必要的套件。"
      ],
      "metadata": {
        "id": "SEO3FrtDr19l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "nQsFwgqer9UG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "請不要更改 random seed !!!!"
      ],
      "metadata": {
        "id": "keIM9OL4r-92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 543 # Do not change this\n",
        "def fix(env, seed):\n",
        "  obs, info = env.reset(seed=seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "kl416gzSsCsu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後，引入 OpenAI 的 gym，並建立一個 Lunar Lander 環境。"
      ],
      "metadata": {
        "id": "9GVFvdubsFF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")\n",
        "\n",
        "fix(env, seed)\n",
        "\n",
        "import time\n",
        "start = time.time()"
      ],
      "metadata": {
        "id": "BDAK6QbpsHSl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwUb6VsQsNnl",
        "outputId": "839e2729-5574-461a-f0f9-4593c5755659"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==1.7.0\n",
            "aiofiles==24.1.0\n",
            "aiohappyeyeballs==2.6.1\n",
            "aiohttp==3.11.15\n",
            "aiosignal==1.3.2\n",
            "alabaster==1.0.0\n",
            "albucore==0.0.24\n",
            "albumentations==2.0.8\n",
            "ale-py==0.11.1\n",
            "altair==5.5.0\n",
            "annotated-types==0.7.0\n",
            "antlr4-python3-runtime==4.9.3\n",
            "anyio==4.9.0\n",
            "argon2-cffi==25.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.7.2\n",
            "arviz==0.21.0\n",
            "astropy==7.1.0\n",
            "astropy-iers-data==0.2025.6.16.0.38.47\n",
            "astunparse==1.6.3\n",
            "atpublic==5.1\n",
            "attrs==25.3.0\n",
            "audioread==3.0.1\n",
            "autograd==1.8.0\n",
            "babel==2.17.0\n",
            "backcall==0.2.0\n",
            "backports.tarfile==1.2.0\n",
            "beautifulsoup4==4.13.4\n",
            "betterproto==2.0.0b6\n",
            "bigframes==2.6.0\n",
            "bigquery-magics==0.9.0\n",
            "bleach==6.2.0\n",
            "blinker==1.9.0\n",
            "blis==1.3.0\n",
            "blobfile==3.0.0\n",
            "blosc2==3.4.0\n",
            "bokeh==3.7.3\n",
            "Bottleneck==1.4.2\n",
            "box2d-py==2.3.8\n",
            "bqplot==0.12.45\n",
            "branca==0.8.1\n",
            "build==1.2.2.post1\n",
            "CacheControl==0.14.3\n",
            "cachetools==5.5.2\n",
            "catalogue==2.0.10\n",
            "certifi==2025.6.15\n",
            "cffi==1.17.1\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.4.2\n",
            "chex==0.1.89\n",
            "clarabel==0.11.1\n",
            "click==8.2.1\n",
            "cloudpathlib==0.21.1\n",
            "cloudpickle==3.1.1\n",
            "cmake==3.31.6\n",
            "cmdstanpy==1.2.5\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contourpy==1.3.2\n",
            "cramjam==2.10.0\n",
            "cryptography==43.0.3\n",
            "cuda-python==12.6.2.post1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "cudf-polars-cu12==25.2.2\n",
            "cufflinks==0.17.3\n",
            "cuml-cu12==25.2.1\n",
            "cupy-cuda12x==13.3.0\n",
            "curl_cffi==0.11.3\n",
            "cuvs-cu12==25.2.1\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.6.6\n",
            "cycler==0.12.1\n",
            "cyipopt==1.5.0\n",
            "cymem==2.0.11\n",
            "Cython==3.0.12\n",
            "dask==2024.12.1\n",
            "dask-cuda==25.2.0\n",
            "dask-cudf-cu12==25.2.2\n",
            "dask-expr==1.1.21\n",
            "dataproc-spark-connect==0.7.5\n",
            "datascience==0.17.6\n",
            "datasets==2.14.4\n",
            "db-dtypes==1.4.3\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.8.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "diffusers==0.33.1\n",
            "dill==0.3.7\n",
            "distributed==2024.12.1\n",
            "distributed-ucxx-cu12==0.42.0\n",
            "distro==1.9.0\n",
            "dlib==19.24.6\n",
            "dm-tree==0.1.9\n",
            "docstring_parser==0.16\n",
            "docutils==0.21.2\n",
            "dopamine_rl==4.1.2\n",
            "duckdb==1.2.2\n",
            "earthengine-api==1.5.19\n",
            "easydict==1.13\n",
            "editdistance==0.8.1\n",
            "eerepr==0.1.2\n",
            "einops==0.8.1\n",
            "en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85\n",
            "entrypoints==0.4\n",
            "et_xmlfile==2.0.0\n",
            "etils==1.12.2\n",
            "etuples==0.3.9\n",
            "Farama-Notifications==0.0.4\n",
            "fastai==2.7.19\n",
            "fastapi==0.115.12\n",
            "fastcore==1.7.29\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.21.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.3\n",
            "ffmpy==0.6.0\n",
            "filelock==3.18.0\n",
            "firebase-admin==6.9.0\n",
            "Flask==3.1.1\n",
            "flatbuffers==25.2.10\n",
            "flax==0.10.6\n",
            "folium==0.19.7\n",
            "fonttools==4.58.4\n",
            "frozendict==2.4.6\n",
            "frozenlist==1.7.0\n",
            "fsspec==2025.3.2\n",
            "future==1.0.0\n",
            "gast==0.6.0\n",
            "gcsfs==2025.3.2\n",
            "GDAL==3.8.4\n",
            "gdown==5.2.0\n",
            "geemap==0.35.3\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==1.0.1\n",
            "geopy==2.4.1\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.12\n",
            "GitPython==3.1.44\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.15\n",
            "google-api-core==2.25.1\n",
            "google-api-python-client==2.172.0\n",
            "google-auth==2.38.0\n",
            "google-auth-httplib2==0.2.0\n",
            "google-auth-oauthlib==1.2.2\n",
            "google-cloud-aiplatform==1.97.0\n",
            "google-cloud-bigquery==3.34.0\n",
            "google-cloud-bigquery-connection==1.18.3\n",
            "google-cloud-bigquery-storage==2.32.0\n",
            "google-cloud-core==2.4.3\n",
            "google-cloud-dataproc==5.20.0\n",
            "google-cloud-datastore==2.21.0\n",
            "google-cloud-firestore==2.21.0\n",
            "google-cloud-functions==1.20.4\n",
            "google-cloud-iam==2.19.1\n",
            "google-cloud-language==2.17.2\n",
            "google-cloud-resource-manager==1.14.2\n",
            "google-cloud-spanner==3.55.0\n",
            "google-cloud-storage==2.19.0\n",
            "google-cloud-translate==3.20.3\n",
            "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
            "google-crc32c==1.7.1\n",
            "google-genai==1.20.0\n",
            "google-generativeai==0.8.5\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.2\n",
            "googleapis-common-protos==1.70.0\n",
            "googledrivedownloader==1.1.0\n",
            "gradio==5.31.0\n",
            "gradio_client==1.10.1\n",
            "graphviz==0.21\n",
            "greenlet==3.2.3\n",
            "groovy==0.1.2\n",
            "grpc-google-iam-v1==0.14.2\n",
            "grpc-interceptor==0.15.4\n",
            "grpcio==1.73.0\n",
            "grpcio-status==1.71.0\n",
            "grpclib==0.4.8\n",
            "gspread==6.2.1\n",
            "gspread-dataframe==4.0.0\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "gymnasium==1.1.1\n",
            "h11==0.16.0\n",
            "h2==4.2.0\n",
            "h5netcdf==1.6.1\n",
            "h5py==3.14.0\n",
            "hdbscan==0.8.40\n",
            "hf-xet==1.1.3\n",
            "hf_transfer==0.1.9\n",
            "highspy==1.11.0\n",
            "holidays==0.74\n",
            "holoviews==1.20.2\n",
            "hpack==4.1.0\n",
            "html5lib==1.1\n",
            "httpcore==1.0.9\n",
            "httpimport==1.4.1\n",
            "httplib2==0.22.0\n",
            "httpx==0.28.1\n",
            "huggingface-hub==0.33.0\n",
            "humanize==4.12.3\n",
            "hyperframe==6.1.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==9.5.0\n",
            "idna==3.10\n",
            "imageio==2.37.0\n",
            "imageio-ffmpeg==0.6.0\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.13.0\n",
            "immutabledict==4.2.1\n",
            "importlib_metadata==8.7.0\n",
            "importlib_resources==6.5.2\n",
            "imutils==0.5.4\n",
            "inflect==7.5.0\n",
            "iniconfig==2.1.0\n",
            "intel-cmplr-lib-ur==2025.1.1\n",
            "intel-openmp==2025.1.1\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==6.17.1\n",
            "ipyleaflet==0.20.0\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jaraco.classes==3.4.0\n",
            "jaraco.context==6.0.1\n",
            "jaraco.functools==4.1.0\n",
            "jax==0.5.2\n",
            "jax-cuda12-pjrt==0.5.1\n",
            "jax-cuda12-plugin==0.5.1\n",
            "jaxlib==0.5.1\n",
            "jeepney==0.9.0\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.6\n",
            "jiter==0.10.0\n",
            "joblib==1.5.1\n",
            "jsonpatch==1.33\n",
            "jsonpickle==4.1.1\n",
            "jsonpointer==3.0.0\n",
            "jsonschema==4.24.0\n",
            "jsonschema-specifications==2025.4.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-leaflet==0.20.0\n",
            "jupyter-server==1.16.0\n",
            "jupyter_core==5.8.1\n",
            "jupyter_kernel_gateway @ git+https://github.com/googlecolab/kernel_gateway@b134e9945df25c2dcb98ade9129399be10788671\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.15\n",
            "jupytext==1.17.2\n",
            "kaggle==1.7.4.5\n",
            "kagglehub==0.3.12\n",
            "keras==3.8.0\n",
            "keras-hub==0.18.1\n",
            "keras-nlp==0.18.1\n",
            "keyring==25.6.0\n",
            "keyrings.google-artifactregistry-auth==1.1.2\n",
            "kiwisolver==1.4.8\n",
            "langchain==0.3.25\n",
            "langchain-core==0.3.65\n",
            "langchain-text-splitters==0.3.8\n",
            "langcodes==3.5.0\n",
            "langsmith==0.3.45\n",
            "language_data==1.3.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl\n",
            "libcugraph-cu12==25.2.0\n",
            "libcuml-cu12==25.2.1\n",
            "libcuvs-cu12==25.2.1\n",
            "libkvikio-cu12==25.2.1\n",
            "libpysal==4.13.0\n",
            "libraft-cu12==25.2.0\n",
            "librosa==0.11.0\n",
            "libucx-cu12==1.18.1\n",
            "libucxx-cu12==0.42.0\n",
            "lightgbm @ file:///tmp/lightgbm/LightGBM/dist/lightgbm-4.5.0-py3-none-linux_x86_64.whl\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.43.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==5.4.0\n",
            "Mako==1.1.3\n",
            "marisa-trie==1.2.1\n",
            "Markdown==3.8\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==3.0.2\n",
            "matplotlib==3.10.0\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==1.1.2\n",
            "mdit-py-plugins==0.4.2\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==3.1.3\n",
            "mizani==0.13.5\n",
            "mkl==2025.0.1\n",
            "ml-dtypes==0.4.1\n",
            "mlxtend==0.23.4\n",
            "more-itertools==10.7.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.1.1\n",
            "multidict==6.4.4\n",
            "multipledispatch==1.0.0\n",
            "multiprocess==0.70.15\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.13\n",
            "music21==9.3.0\n",
            "namex==0.1.0\n",
            "narwhals==1.43.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.3.1\n",
            "nbclient==0.10.2\n",
            "nbconvert==7.16.6\n",
            "nbformat==5.10.4\n",
            "ndindex==1.10.0\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.5\n",
            "nibabel==5.3.2\n",
            "nltk==3.9.1\n",
            "notebook==6.5.7\n",
            "notebook_shim==0.2.4\n",
            "numba==0.60.0\n",
            "numba-cuda==0.2.0\n",
            "numexpr==2.11.0\n",
            "numpy==2.0.2\n",
            "nvidia-cublas-cu12==12.4.5.8\n",
            "nvidia-cuda-cupti-cu12==12.4.127\n",
            "nvidia-cuda-nvcc-cu12==12.5.82\n",
            "nvidia-cuda-nvrtc-cu12==12.4.127\n",
            "nvidia-cuda-runtime-cu12==12.4.127\n",
            "nvidia-cudnn-cu12==9.1.0.70\n",
            "nvidia-cufft-cu12==11.2.1.3\n",
            "nvidia-curand-cu12==10.3.5.147\n",
            "nvidia-cusolver-cu12==11.6.1.9\n",
            "nvidia-cusparse-cu12==12.3.1.170\n",
            "nvidia-cusparselt-cu12==0.6.2\n",
            "nvidia-ml-py==12.575.51\n",
            "nvidia-nccl-cu12==2.21.5\n",
            "nvidia-nvcomp-cu12==4.2.0.11\n",
            "nvidia-nvjitlink-cu12==12.4.127\n",
            "nvidia-nvtx-cu12==12.4.127\n",
            "nvtx==0.2.12\n",
            "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "omegaconf==2.3.0\n",
            "openai==1.86.0\n",
            "opencv-contrib-python==4.11.0.86\n",
            "opencv-python==4.11.0.86\n",
            "opencv-python-headless==4.11.0.86\n",
            "openpyxl==3.1.5\n",
            "opt_einsum==3.4.0\n",
            "optax==0.2.5\n",
            "optree==0.16.0\n",
            "orbax-checkpoint==0.11.15\n",
            "orjson==3.10.18\n",
            "osqp==1.0.4\n",
            "packaging==24.2\n",
            "pandas==2.2.2\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.29.1\n",
            "pandas-stubs==2.2.2.240909\n",
            "pandocfilters==1.5.1\n",
            "panel==1.7.1\n",
            "param==2.2.1\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==1.0.1\n",
            "peewee==3.18.1\n",
            "peft==0.15.2\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "pillow==11.2.1\n",
            "platformdirs==4.3.8\n",
            "plotly==5.24.1\n",
            "plotnine==0.14.5\n",
            "pluggy==1.6.0\n",
            "ply==3.11\n",
            "polars==1.21.0\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "preshed==3.0.10\n",
            "prettytable==3.16.0\n",
            "proglog==0.1.12\n",
            "progressbar2==4.5.0\n",
            "prometheus_client==0.22.1\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.51\n",
            "propcache==0.3.2\n",
            "prophet==1.1.7\n",
            "proto-plus==1.26.1\n",
            "protobuf==5.29.5\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.10\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==18.1.0\n",
            "pyasn1==0.6.1\n",
            "pyasn1_modules==0.4.2\n",
            "pycairo==1.28.0\n",
            "pycocotools==2.0.10\n",
            "pycparser==2.22\n",
            "pycryptodomex==3.23.0\n",
            "pydantic==2.11.7\n",
            "pydantic_core==2.33.2\n",
            "pydata-google-auth==1.9.1\n",
            "pydot==3.0.4\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.21.3\n",
            "pydub==0.25.1\n",
            "pyerfa==2.0.1.5\n",
            "pygame==2.6.1\n",
            "pygit2==1.18.0\n",
            "Pygments==2.19.1\n",
            "PyGObject==3.42.0\n",
            "PyJWT==2.10.1\n",
            "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "pylibcugraph-cu12==25.2.0\n",
            "pylibraft-cu12==25.2.0\n",
            "pymc==5.23.0\n",
            "pymystem3==0.2.0\n",
            "pynndescent==0.5.13\n",
            "pynvjitlink-cu12==0.6.0\n",
            "pynvml==12.0.0\n",
            "pyogrio==0.11.0\n",
            "pyomo==6.9.2\n",
            "PyOpenGL==3.1.9\n",
            "pyOpenSSL==24.2.1\n",
            "pyparsing==3.2.3\n",
            "pyperclip==1.9.0\n",
            "pyproj==3.7.1\n",
            "pyproject_hooks==1.2.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pyspark==3.5.1\n",
            "pytensor==2.31.3\n",
            "pytest==8.3.5\n",
            "python-apt==0.0.0\n",
            "python-box==7.3.2\n",
            "python-dateutil==2.9.0.post0\n",
            "python-louvain==0.16\n",
            "python-multipart==0.0.20\n",
            "python-slugify==8.0.4\n",
            "python-snappy==0.7.3\n",
            "python-utils==3.9.1\n",
            "pytz==2025.2\n",
            "PyVirtualDisplay==3.0\n",
            "pyviz_comms==3.0.5\n",
            "PyWavelets==1.8.0\n",
            "PyYAML==6.0.2\n",
            "pyzmq==24.0.1\n",
            "raft-dask-cu12==25.2.0\n",
            "rapids-dask-dependency==25.2.0\n",
            "ratelim==0.1.6\n",
            "referencing==0.36.2\n",
            "regex==2024.11.6\n",
            "requests==2.32.3\n",
            "requests-oauthlib==2.0.0\n",
            "requests-toolbelt==1.0.0\n",
            "requirements-parser==0.9.0\n",
            "rich==13.9.4\n",
            "rmm-cu12==25.2.0\n",
            "roman-numerals-py==3.1.0\n",
            "rpds-py==0.25.1\n",
            "rpy2==3.5.17\n",
            "rsa==4.9.1\n",
            "ruff==0.11.13\n",
            "safehttpx==0.1.6\n",
            "safetensors==0.5.3\n",
            "scikit-image==0.25.2\n",
            "scikit-learn==1.6.1\n",
            "scipy==1.15.3\n",
            "scooby==0.10.1\n",
            "scs==3.2.7.post2\n",
            "seaborn==0.13.2\n",
            "SecretStorage==3.3.3\n",
            "semantic-version==2.10.0\n",
            "Send2Trash==1.8.3\n",
            "sentence-transformers==4.1.0\n",
            "sentencepiece==0.2.0\n",
            "sentry-sdk==2.30.0\n",
            "setproctitle==1.3.6\n",
            "shap==0.48.0\n",
            "shapely==2.1.1\n",
            "shellingham==1.5.4\n",
            "simple-parsing==0.1.7\n",
            "simplejson==3.20.1\n",
            "simsimd==6.4.9\n",
            "six==1.17.0\n",
            "sklearn-compat==0.1.3\n",
            "sklearn-pandas==2.2.0\n",
            "slicer==0.0.8\n",
            "smart-open==7.1.0\n",
            "smmap==5.0.2\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==3.0.1\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.13.1\n",
            "soupsieve==2.7\n",
            "soxr==0.5.0.post1\n",
            "spacy==3.8.7\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "spanner-graph-notebook==1.1.7\n",
            "Sphinx==8.2.3\n",
            "sphinxcontrib-applehelp==2.0.0\n",
            "sphinxcontrib-devhelp==2.0.0\n",
            "sphinxcontrib-htmlhelp==2.1.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==2.0.0\n",
            "sphinxcontrib-serializinghtml==2.0.0\n",
            "SQLAlchemy==2.0.41\n",
            "sqlglot==25.20.2\n",
            "sqlparse==0.5.3\n",
            "srsly==2.5.1\n",
            "stanio==0.5.1\n",
            "starlette==0.46.2\n",
            "statsmodels==0.14.4\n",
            "stringzilla==3.12.5\n",
            "stumpy==1.13.0\n",
            "swig==4.3.1\n",
            "sympy==1.13.1\n",
            "tables==3.10.2\n",
            "tabulate==0.9.0\n",
            "tbb==2022.1.0\n",
            "tblib==3.1.0\n",
            "tcmlib==1.3.0\n",
            "tenacity==9.1.2\n",
            "tensorboard==2.18.0\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.18.0\n",
            "tensorflow-datasets==4.9.9\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.17.1\n",
            "tensorflow-probability==0.25.0\n",
            "tensorflow-text==2.18.1\n",
            "tensorflow_decision_forests==1.11.0\n",
            "tensorstore==0.1.74\n",
            "termcolor==3.1.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.19.0\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.18.0\n",
            "thinc==8.3.6\n",
            "threadpoolctl==3.6.0\n",
            "tifffile==2025.6.11\n",
            "tiktoken==0.9.0\n",
            "timm==1.0.15\n",
            "tinycss2==1.4.0\n",
            "tokenizers==0.21.1\n",
            "toml==0.10.2\n",
            "tomlkit==0.13.3\n",
            "toolz==0.12.1\n",
            "torch @ https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "torchao==0.10.0\n",
            "torchaudio @ https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "torchdata==0.11.0\n",
            "torchsummary==1.5.1\n",
            "torchtune==0.6.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "tornado==6.4.2\n",
            "tqdm==4.67.1\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.52.4\n",
            "treelite==4.4.1\n",
            "treescope==0.1.9\n",
            "triton==3.2.0\n",
            "tsfresh==0.21.0\n",
            "tweepy==4.15.0\n",
            "typeguard==4.4.3\n",
            "typer==0.16.0\n",
            "types-pytz==2025.2.0.20250516\n",
            "types-setuptools==80.9.0.20250529\n",
            "typing-inspection==0.4.1\n",
            "typing_extensions==4.14.0\n",
            "tzdata==2025.2\n",
            "tzlocal==5.3.1\n",
            "uc-micro-py==1.0.3\n",
            "ucx-py-cu12==0.42.0\n",
            "ucxx-cu12==0.42.0\n",
            "umap-learn==0.5.7\n",
            "umf==0.10.0\n",
            "uritemplate==4.2.0\n",
            "urllib3==2.4.0\n",
            "uvicorn==0.34.3\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wandb==0.20.1\n",
            "wasabi==1.1.3\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.11.1\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "websockets==15.0.1\n",
            "Werkzeug==3.1.3\n",
            "widgetsnbextension==3.6.10\n",
            "wordcloud==1.9.4\n",
            "wrapt==1.17.2\n",
            "wurlitzer==3.1.1\n",
            "xarray==2025.3.1\n",
            "xarray-einstats==0.9.0\n",
            "xgboost==2.1.4\n",
            "xlrd==2.0.2\n",
            "xxhash==3.5.0\n",
            "xyzservices==2025.4.0\n",
            "yarl==1.20.1\n",
            "ydf==0.12.0\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.63\n",
            "zict==3.0.0\n",
            "zipp==3.23.0\n",
            "zstandard==0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 什麼是 Lunar Lander？\n",
        "“LunarLander-v2” 這個環境是在模擬登月小艇降落在月球表面時的情形。 這個任務的目標是讓登月小艇「安全地」降落在兩個黃色旗幟間的平地上。\n",
        "\n",
        "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector.\n",
        "\n",
        "所謂的「環境」其實同時包括了 agent 和 environment。 我們利用 step() 這個函式讓 agent 行動，而後函式便會回傳 environment 給予的 observation/state（以下這兩個名詞代表同樣的意思）和 reward。"
      ],
      "metadata": {
        "id": "Q383CwmlsR0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Observation / State\n",
        "首先，我們可以看看 environment 回傳給 agent 的 observation 究竟是長什麼樣子的資料："
      ],
      "metadata": {
        "id": "58yoo59asuB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.observation_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sALkiyjJsvkt",
        "outputId": "086a1918-9c31-4bef-b0e0-9933ef251a5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Box(8,)` 說明我們會拿到 8 維的向量作為 observation，其中包含：垂直及水平座標、速度、角度、加速度等等，這部分我們就不細說。\n",
        "\n",
        "# Action\n",
        "而在 agent 得到 observation 和 reward 以後，能夠採取的動作有："
      ],
      "metadata": {
        "id": "DiYrATkGszTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.action_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZdvYC0vs4md",
        "outputId": "d5e61cb1-8415-48f1-cd06-820868f148cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Discrete(4)` 說明 agent 可以採取四種離散的行動：\n",
        "\n",
        "- 0 代表不採取任何行動\n",
        "- 2 代表主引擎向下噴射\n",
        "- 1, 3 則是向左右噴射\n",
        "\n",
        "接下來，我們嘗試讓 agent 與 environment 互動。 在進行任何操作前，建議先呼叫 `reset()` 函式讓整個「環境」重置。 而這個函式同時會回傳「環境」最初始的狀態。"
      ],
      "metadata": {
        "id": "GLYa_M_Ss9Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_observation, info = env.reset(seed=seed)\n",
        "print(initial_observation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av8ijKOLtJWF",
        "outputId": "9330ef37-cd7b-4eb6-beda-ffb3c23693c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.2619973e-03  1.3984586e+00 -1.2784091e-01 -5.5384123e-01\n",
            "  1.4691149e-03  2.8957864e-02  0.0000000e+00  0.0000000e+00]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "接著，我們試著從 agent 的四種行動空間中，隨機採取一個行動"
      ],
      "metadata": {
        "id": "gwgrlzEytMnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_action = env.action_space.sample()\n",
        "print(random_action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B5FQWOCtNM9",
        "outputId": "13d696e3-56f5-4b43-de44-ae18aade0553"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "再利用 `step()` 函式讓 agent 根據我們隨機抽樣出來的 `random_action` 動作。 而這個函式會回傳四項資訊：\n",
        "\n",
        "- observation / state\n",
        "- reward\n",
        "- 自然完成\n",
        "- 超時或中斷\n",
        "- 其餘資訊"
      ],
      "metadata": {
        "id": "thjMlNZ-tQHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation, reward, terminated, truncated, info = env.step(random_action)"
      ],
      "metadata": {
        "id": "YHiY3Ne3tbNV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "第一項資訊 `observation` 即為 agent 採取行動之後，agent 對於環境的 observation 或者說環境的 state 為何。 而第三項資訊 `terminated` 則是 `True` 或 `False` 的布林值，當登月小艇成功著陸或是不幸墜毀時，代表這個回合（episode）也就跟著結束了，此時 `step()` 函式便會回傳 `done = True`，而在那之前，`done` 則保持 `False`。"
      ],
      "metadata": {
        "id": "sEa_spiztdVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(terminated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_NvdSQrtuP1",
        "outputId": "653c672e-52b3-44a4-8045-25e4a78bed34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reward\n",
        "而「環境」給予的 reward 大致是這樣計算：\n",
        "\n",
        "- 小艇墜毀得到 -100 分\n",
        "- 小艇在黃旗幟之間成功著地則得 100~140 分\n",
        "- 噴射主引擎（向下噴火）每次 -0.3 分\n",
        "- 小艇最終完全靜止則再得 100 分\n",
        "- 小艇每隻腳碰觸地面 +10 分\n",
        "\n",
        "> Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame."
      ],
      "metadata": {
        "id": "aInaBePjtzXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(reward) # after doing a random action (0), the immediate reward is stored in this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKZZqQect_GN",
        "outputId": "0fc32e40-01fd-4cb6-ae5a-af91cd9e49cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.0511407416545058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Agent\n",
        "最後，在進入實做之前，我們就來看看這樣一個 random agent 能否成功登陸月球："
      ],
      "metadata": {
        "id": "jn8MYd0luEcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset(seed=seed)\n",
        "\n",
        "img = plt.imshow(env.render())\n",
        "\n",
        "terminated = False\n",
        "while not terminated:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "    img.set_data(env.render())\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "z8qt_AFWuFyd",
        "outputId": "ac571db2-bf28-49f3-94cd-15816988b707"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradient\n",
        "現在來搭建一個簡單的 policy network。 我們預設模型的輸入是 8-dim 的 observation，輸出則是離散的四個動作之一："
      ],
      "metadata": {
        "id": "r883Jc7FuKjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyReinforcementNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(8, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 4),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, observation):\n",
        "        action = self.net(observation)\n",
        "        return action"
      ],
      "metadata": {
        "id": "luSxFz1KuOo1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "再來，搭建一個簡單的 agent，並搭配上方的 policy network 來採取行動。 這個 agent 能做到以下幾件事：\n",
        "\n",
        "- `learn()`：從記下來的 log probabilities 及 rewards 來更新 policy network。\n",
        "- `sample()`：從 environment 得到 observation 之後，利用 policy network 得出應該採取的行動。 而此函式除了回傳抽樣出來的 action，也會回傳此次抽樣的 log probabilities。"
      ],
      "metadata": {
        "id": "0vZQjHTEJSFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyAgent():\n",
        "    def __init__(self, network):\n",
        "        self.network = network\n",
        "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)\n",
        "\n",
        "    def learn(self, log_probs, rewards):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = (-log_probs * rewards).sum()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def sample(self, observation):\n",
        "        action_prob = self.network(torch.tensor(observation))\n",
        "        action_dist = Categorical(action_prob)\n",
        "        action = action_dist.sample()\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        return action.item(), log_prob"
      ],
      "metadata": {
        "id": "33ZYSN4hJbBo"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後，建立一個 network 和 agent，就可以開始進行訓練了"
      ],
      "metadata": {
        "id": "buxm1WBh2cxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reinforcement_network = MyReinforcementNetwork()\n",
        "agent = MyAgent(reinforcement_network)"
      ],
      "metadata": {
        "id": "5A888Y591_XP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 訓練 Agent\n",
        "現在我們開始訓練 agent。 透過讓 agent 和 environment 互動，我們記住每一組對應的 log probabilities 及 reward，並在成功登陸或者不幸墜毀後，回放這些「記憶」來訓練 policy network。"
      ],
      "metadata": {
        "id": "ZZ9TLiiV2f_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.network.train()\n",
        "EPISODE_PER_BATCH = 5\n",
        "NUM_BATCH = 400\n",
        "GAMMA = 0.99\n",
        "\n",
        "avg_total_rewards = []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "\n",
        "        one_episode_reward = []\n",
        "        observation, _ = env.reset(seed=seed)\n",
        "        total_step = 0\n",
        "\n",
        "        while True:\n",
        "            action, log_prob = agent.sample(observation)\n",
        "            observation, reward, terminated, truncated, _ = env.step(action)\n",
        "            log_probs.append(log_prob)\n",
        "            one_episode_reward.append(reward)\n",
        "            #total_reward += reward\n",
        "            total_step += 1\n",
        "\n",
        "            done = terminated or truncated\n",
        "            if done:\n",
        "                weighted_reward = 0\n",
        "                final_reward = []\n",
        "                for r in reversed(one_episode_reward):\n",
        "                    weighted_reward = GAMMA * weighted_reward + r\n",
        "                    final_reward.insert(0, weighted_reward)\n",
        "                rewards.extend(final_reward)\n",
        "                break\n",
        "\n",
        "    #print(f\"rewards looks like \", rewards)\n",
        "    #print(f\"log_probs looks like \", log_probs)\n",
        "\n",
        "    batch_reward = torch.tensor(rewards, dtype=torch.float32)\n",
        "    batch_log_prob = torch.stack(log_probs)\n",
        "\n",
        "    # 正規標準化\n",
        "    batch_reward_mean = batch_reward.mean()\n",
        "    batch_reward_std = batch_reward.std() + 1e-9\n",
        "    batch_reward = (batch_reward - batch_reward_mean) / batch_reward_std\n",
        "\n",
        "    print(f\"batch reward mean = \",batch_reward_mean)\n",
        "    print(f\"batch_reward looks like \", batch_reward)\n",
        "    #print(f\"batch_log_prob looks like \", batch_log_prob)\n",
        "    # update actor network\n",
        "    agent.learn(batch_log_prob, batch_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f3a3d37353894381b750fd73b58ec926",
            "fa0f41f6d181417997f3a74d1f1b2a84",
            "75559963e1c44884b78e82352c2a4d8a",
            "d0a1bef3f1284020a0604e4fba68e9bb",
            "e89fa1a0efd3425d82e350e67418e3b6",
            "06907a3c036e415ea95cf985699f7bfa",
            "71304ce571ec4d6d80b64d6de6ed18bd",
            "d9255fe3e37b40a9806ad27bce8b2514",
            "47fa2ebc4ddf4e30afc052cf9431537d",
            "5adc140cf4f44aa19e8f2675168b9343",
            "10478e8253894c92ac8120ee2d9073e1"
          ]
        },
        "id": "d5dymchU2icR",
        "outputId": "d00e7fe5-4050-457a-a5ed-074ae02ee883"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/400 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3a3d37353894381b750fd73b58ec926"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m串流輸出內容已截斷至最後 5000 行。\u001b[0m\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n",
            "batch reward mean =  tensor(-216.8488)\n",
            "batch_reward looks like  tensor([-0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654,\n",
            "        -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638,\n",
            "        -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346,\n",
            "        -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359,\n",
            "        -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,\n",
            "         0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,\n",
            "         1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781, -0.8109, -0.8330,\n",
            "        -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940, -0.8921, -0.8858,\n",
            "        -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840, -0.7528, -0.7177,\n",
            "        -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204, -0.3577, -0.2866,\n",
            "        -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,  0.3246,  0.4358,\n",
            "         0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,  1.3197,  1.4647,\n",
            "         1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407, -0.7470, -0.7781,\n",
            "        -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877, -0.8930, -0.8940,\n",
            "        -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313, -0.8091, -0.7840,\n",
            "        -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384, -0.4803, -0.4204,\n",
            "        -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,  0.1215,  0.2212,\n",
            "         0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,  1.0397,  1.1749,\n",
            "         1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,  2.2523,  2.4407,\n",
            "        -0.7470, -0.7781, -0.8109, -0.8330, -0.8500, -0.8654, -0.8776, -0.8877,\n",
            "        -0.8930, -0.8940, -0.8921, -0.8858, -0.8764, -0.8638, -0.8491, -0.8313,\n",
            "        -0.8091, -0.7840, -0.7528, -0.7177, -0.6785, -0.6346, -0.5881, -0.5384,\n",
            "        -0.4803, -0.4204, -0.3577, -0.2866, -0.2120, -0.1359, -0.0581,  0.0272,\n",
            "         0.1215,  0.2212,  0.3246,  0.4358,  0.5467,  0.6596,  0.7814,  0.9051,\n",
            "         1.0397,  1.1749,  1.3197,  1.4647,  1.6127,  1.7629,  1.9186,  2.0801,\n",
            "         2.2523,  2.4407])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 訓練結果\n",
        "訓練過程中，我們持續記下了 `avg_total_reward`，這個數值代表的是：每次更新 policy network 前，我們讓 agent 玩數個回合（episodes），而這些回合的平均 total rewards 為何。 理論上，若是 agent 一直在進步，則所得到的 avg_total_reward 也會持續上升，直至 250 上下。 若將其畫出來則結果如下："
      ],
      "metadata": {
        "id": "Sl70_IIi5KoM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Zi2Jrd65nMt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}