{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcXgfmr+KYuRDCJ2dlUOaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lollipop6370/ML2021/blob/main/ML2021_HW04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset"
      ],
      "metadata": {
        "id": "iOLDI4at5fQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 'paste your own data download link' --output Dataset.zip # 1gaFy8RaQVUEXo2n0peCBR5gYKCB-mNHc\n",
        "!unzip Dataset.zip"
      ],
      "metadata": {
        "id": "7HpDZ7mv5js4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages\n"
      ],
      "metadata": {
        "id": "JQSpG01yJdgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AnqER8OrJYB6"
      },
      "outputs": [],
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import csv\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "myseed = 42069  # set a random seed for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(myseed)\n",
        "torch.manual_seed(myseed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Utilities"
      ],
      "metadata": {
        "id": "vRV7cDq1JnOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "IMD97Q1vJc7V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "HZxIoUCyKTY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, path, segment_len=128):\n",
        "        self.path = path\n",
        "        self.segment_len = segment_len\n",
        "\n",
        "        # Load the mapping from speaker name to their corresponding id.\n",
        "        dataPath = Path(self.path) / \"mapping.json\"\n",
        "        with dataPath.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            mapping = json.load(f)\n",
        "            self.speaker2id = mapping[\"speaker2id\"]\n",
        "\n",
        "        # Load metadata of training data\n",
        "        metadata_path = Path(self.path) / \"metadata.json\"\n",
        "        with metadata_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            metadata = json.load(f)[\"speakers\"]\n",
        "\n",
        "        # Get the total number of speaker.\n",
        "        self.speaker_num = len(metadata.keys())\n",
        "        self.data = []\n",
        "        for speaker in metadata.keys():\n",
        "            for soundFile in metadata[speaker]:\n",
        "                self.data.append([soundFile[\"feature_path\"], self.speaker2id[speaker]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        feat_path, speaker = self.data[index]\n",
        "        # Load sound file\n",
        "        mel = torch.load(Path(self.path) / feat_path)\n",
        "\n",
        "        # Segment mel-spectrogram into \"segment_len\" frames.\n",
        "        if len(mel) > self.segment_len:\n",
        "            start = random.randint(0, len(mel) - self.segment_len)\n",
        "            mel = torch.FloatTensor(mel[start:start+self.segment_len])\n",
        "        else:\n",
        "            mel = torch.FloatTensor(mel)\n",
        "        #\n",
        "        speaker = torch.FloatTensor([speaker]).long()\n",
        "        return mel, speaker\n",
        "\n",
        "    def get_speaker_number(self):\n",
        "        return self.speaker_num"
      ],
      "metadata": {
        "id": "MW8AOeuFKSPc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "PmJhL8sLmFbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    # let [batch1, batch2] to mel, speaker = [batch1.mel, batch2.mel], [batch1.speaker, batch2.speaker]\n",
        "    mel, speaker = zip(*batch)\n",
        "    # pad the features in the same batch to make their length the same.\n",
        "    mel = pad_sequence(mel, batch_first=True, padding_value=-20) # new tensor shap: (batch_size, length, 40)\n",
        "\n",
        "    return mel, torch.LongTensor(speaker)\n",
        "\n",
        "def getDataLoader(path, batch_size, n_workers):\n",
        "    # Create Dataset\n",
        "    myDataset = MyDataset(path)\n",
        "    speaker_num = myDataset.get_speaker_number()\n",
        "    # Random split dataset into training dataset and validation dataset.\n",
        "    trainSetLen = int(0.9 * len(myDataset))\n",
        "    trainSet, validSet = random_split(myDataset, [trainSetLen, len(myDataset) - trainSetLen])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        trainSet,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        num_workers=n_workers,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        validSet,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=n_workers,\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_batch\n",
        "    )\n",
        "\n",
        "    return train_loader, valid_loader, speaker_num"
      ],
      "metadata": {
        "id": "loCQq9aCmH82"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "9R3-uK-HqKLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if use conformer\n",
        "from torchaudio.models import Conformer\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # 40 features transformation to 80  (batch size, lenght, 40) -> (batch size, lenght, 80)\n",
        "        self.prenet = nn.Linear(40, d_model)\n",
        "\n",
        "        \"\"\" Use conformer\n",
        "        self.conformer = Conformer(\n",
        "            input_dim=d_model,\n",
        "            num_heads=2,\n",
        "            ffn_dim=256,\n",
        "            num_layers=2,\n",
        "            depthwise_conv_kernel_size=15,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        # transformer's encoder\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            dim_feedforward=256,\n",
        "            nhead=2\n",
        "        )\n",
        "\n",
        "        # self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)\n",
        "\n",
        "        # project the dimention of feature from d_model into speaker nums.\n",
        "        self.pred_layer = nn.Sequential(\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(d_model, n_spks)\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, mels):\n",
        "        \"\"\"\n",
        "        args:\n",
        "          mels: (batch size, sequence length, feature=40)\n",
        "        return:\n",
        "          out: (batch size, n_spks)\n",
        "        \"\"\"\n",
        "        out = self.prenet(mels) # out: (batch size, sequence length, 80)\n",
        "        # change dimention 0 and 1 for transformer input.\n",
        "        out = out.permute(1, 0, 2)  # out: (sequence length, batch size, 80)\n",
        "        # transformer encoder layer\n",
        "        out = self.encoder_layer(out) # out: (sequence length, batch size, 80)\n",
        "        # turn dimention back\n",
        "        out = out.transpose(0, 1) # out: (batch size, sequence lenght, 80)\n",
        "        # mean pooling\n",
        "        out = out.mean(dim=1) # out: (batch size, 80)\n",
        "        out = self.pred_layer(out) # out: (batch size, n_spks)\n",
        "        return out\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "        return self.criterion(pred, target)"
      ],
      "metadata": {
        "id": "8YxrmellqM0T"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning rate scheduler"
      ],
      "metadata": {
        "id": "aO8X9Xa1eiN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scheduler(optimizer, num_warmup_steps, num_training_steps, num_cycle=0.5, last_epoch=-1):\n",
        "    def lr_lambda(current_step):\n",
        "        # Warmup\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        # Decade\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycle) * 2.0 * progress)))\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch)"
      ],
      "metadata": {
        "id": "rtxP2auoeqDP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create object"
      ],
      "metadata": {
        "id": "k_hyUWmpDXcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter\n",
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "\n",
        "config = {\n",
        "    'data_dir': './Dataset',\n",
        "    'n_epochs': 70000,                # maximum number of epochs\n",
        "    'batch_size': 32,               # mini-batch size for dataloader\n",
        "    'optimizer': 'AdamW',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 1e-3,                 # learning rate of SGD\n",
        "        # 'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 1000,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.ckpt',  # your model will be saved here\n",
        "    'n_workers': 8,\n",
        "    'warmup_steps': 1000\n",
        "}\n",
        "# DataLoader\n",
        "train_loader, valid_loader, speaker_num = getDataLoader(config['data_dir'], config['batch_size'], config['n_workers'])\n",
        "# Model\n",
        "model = Classifier(n_spks=speaker_num).to(device)\n",
        "# Optimizer\n",
        "optimizer = getattr(torch.optim, config['optimizer'])(model.parameters(), **config['optim_hparas'])\n",
        "# Learning rate scheduler\n",
        "scheduler = get_scheduler(optimizer, num_warmup_steps=config['warmup_steps'], num_training_steps=config['n_epochs'])"
      ],
      "metadata": {
        "id": "Wb49y2O7DcMv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "Uf-9GmNlRK8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataLoader, valid_dataLoader, model, optimizer, n_epoch, device):\n",
        "    min_crossEntropy = 1000.\n",
        "    current_epoch = 0\n",
        "    early_stop = 0\n",
        "    loss_record = {'train': [], 'dev': []}\n",
        "    while(current_epoch < n_epoch):\n",
        "        model.train()\n",
        "        for mel, speaker in train_dataLoader:\n",
        "            optimizer.zero_grad()\n",
        "            mel, speaker = mel.to(device), speaker.to(device)\n",
        "            pred = model(mel)\n",
        "            cross_loss = model.cal_loss(pred, speaker)\n",
        "            cross_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            loss_record['train'].append(cross_loss.detach().cpu().item())\n",
        "\n",
        "        dev_crossEntropy = dev(valid_dataLoader, model, device)\n",
        "        loss_record['dev'].append(dev_crossEntropy)\n",
        "        if dev_crossEntropy < min_crossEntropy:\n",
        "            min_crossEntropy = dev_crossEntropy\n",
        "            print('Saving model (epoch = {:4d}, loss = {:.4f})'.format(current_epoch + 1, min_crossEntropy))\n",
        "            torch.save(model.state_dict(), config['save_path'])\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop += 1\n",
        "\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            break\n",
        "\n",
        "        current_epoch += 1\n",
        "\n",
        "    print('Finished training after {} epochs.'.format(current_epoch))\n",
        "    return min_crossEntropy, loss_record"
      ],
      "metadata": {
        "id": "kBKb3YtcN-EG"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "tZcN9LPTv-7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dev(valid_dataLoader, model, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for mel, speaker in valid_dataLoader:\n",
        "        mel, speaker = mel.to(device), speaker.to(device)\n",
        "        with torch.no_grad():\n",
        "          pred = model(mel)\n",
        "          dev_loss = model.cal_loss(pred, speaker)\n",
        "        total_loss += dev_loss.detach().cpu().item() * len(mel)\n",
        "\n",
        "    total_loss = total_loss / len(valid_dataLoader.dataset)\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "isHPuj_gwFG9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "H0l-MORVMXJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load testing data\n",
        "class Test_dataset(Dataset):\n",
        "    def __init__(self, path, segement_len=128):\n",
        "        self.data_dir = path\n",
        "        self.test_path = Path(path) / \"testdata.json\"\n",
        "        self.segement_len = segement_len\n",
        "        with self.test_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            self.testdata = json.load(f)[\"utterances\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.testdata)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        utterances = self.test_data[index]\n",
        "        feature_path = utterances[\"feature_path\"]\n",
        "        mel = troch.load(Path(self.data_dir) / feature_path)\n",
        "\n",
        "        return feature_path, mel\n",
        "\n",
        "def collate_batch_test(batch):\n",
        "    feature_path, mel = zip(*batch)\n",
        "    return feature_path, torch.stack(mel)\n",
        "\n",
        "def get_test_dataLoader(data_dir, n_workers):\n",
        "    test_dataset = Test_dataset(data_dir)\n",
        "    test_dataLoader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=n_workers,\n",
        "        pin_memory=True,\n",
        "        collate_fn=collate_batch_test\n",
        "    )\n",
        "    return test_dataLoader\n",
        "\n",
        "def test(data_dir, model_path, n_workers, device, output_path):\n",
        "    # Create testdataLoader\n",
        "    test_dataLoader = get_test_dataLoader(data_dir, n_workers)\n",
        "    # Load speaker2id mapping\n",
        "    mapping_path = Path(data_dir) / \"mapping.json\"\n",
        "    mapping = json.load(mapping_path.open())\n",
        "    n_speakers = len(mapping[\"id2speaker\"])\n",
        "    # Load model and parameter(weight)\n",
        "    model = Classifier(n_spks=n_speakers).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    print(\"[Info]: Finished creating model.\")\n",
        "\n",
        "    # Start testing\n",
        "    results = [[\"Id\",\"Category\"]]\n",
        "    for feature_path, mel in test_dataLoader:\n",
        "        with torch.no_gard():\n",
        "            mel = mel.to(device)\n",
        "            pred = model(mel)\n",
        "            pred = pred.argmax(1).cpu().numpy()\n",
        "            results.append([feature_path, mapping[\"id2speaker\"][str(pred)]])\n",
        "\n",
        "    with open(output_path, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerows(results)"
      ],
      "metadata": {
        "id": "B6M7VMufMZwJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start training"
      ],
      "metadata": {
        "id": "OqeJgdF-Li0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss, loss_record = train(train_loader, valid_loader, model, optimizer, config['n_epochs'], device)"
      ],
      "metadata": {
        "id": "hLcWkX3XLnte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "fLL9fq4AMMg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(config[\"data_dir\"], \"./models/model.ckpt\", config[\"n_workers\"], device, \"./output.csv\")"
      ],
      "metadata": {
        "id": "H2V9FvkpMO0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}